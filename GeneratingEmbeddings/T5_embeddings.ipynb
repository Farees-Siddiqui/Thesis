{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import time\n",
    "\n",
    "import sys\n",
    "\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import faiss\n",
    "\n",
    "from transformers import BertTokenizerFast, BertModel, T5TokenizerFast, T5Model\n",
    "from datasets import load_dataset\n",
    "\n",
    "from pprint import pprint\n",
    "import io\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helper import stream\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "len_dataset = 2326839\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5TokenizerFast.from_pretrained('t5-small')\n",
    "model = T5Model.from_pretrained('t5-small').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('json', data_files='dataset/arxiv_data.json', split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.iterable_dataset.IterableDataset at 0x26591560b90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0704.0001',\n",
       " 'submitter': 'Pavel Nadolsky',\n",
       " 'authors': \"C. Bal\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\",\n",
       " 'title': 'Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies',\n",
       " 'comments': '37 pages, 15 figures; published version',\n",
       " 'journal-ref': 'Phys.Rev.D76:013009,2007',\n",
       " 'doi': '10.1103/PhysRevD.76.013009',\n",
       " 'report-no': 'ANL-HEP-PR-07-12',\n",
       " 'categories': 'hep-ph',\n",
       " 'license': None,\n",
       " 'abstract': '  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n',\n",
       " 'versions': [{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'},\n",
       "  {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:27 GMT'}],\n",
       " 'update_date': datetime.datetime(2008, 11, 26, 0, 0),\n",
       " 'authors_parsed': [['Bal√°zs', 'C.', ''],\n",
       "  ['Berger', 'E. L.', ''],\n",
       "  ['Nadolsky', 'P. M.', ''],\n",
       "  ['Yuan', 'C. -P.', '']]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(data):\n",
    "    return tokenizer(data['abstract'], padding=True, truncation=True, max_length=256, return_tensors='pt')\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_dataset, batched=True, batch_size=512, remove_columns=['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'versions', 'update_date', 'authors_parsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(tokenized_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(next(iter(dataloader))['abstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_disk(data, filename):\n",
    "    np.savez(filename, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Model(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous batch took 0.12 seconds\tBatch: 100/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 200/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 300/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 400/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.13 seconds\tBatch: 500/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 600/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 700/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 800/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 900/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 1000/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.13 seconds\tBatch: 1100/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 1200/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 1300/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 1400/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 1500/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 1600/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 1700/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 1800/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 1900/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 2000/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 2100/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 2200/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 2300/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 2400/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 2500/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 2600/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 2700/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 2800/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 2900/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 3000/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 3100/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 3200/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 3300/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 3400/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 3500/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 3600/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 3700/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 3800/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 3900/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 4000/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 4100/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 4200/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 4300/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 4400/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 4500/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 4600/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 4700/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 4800/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 4900/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 5000/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 5100/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 5200/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 5300/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 5400/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 5500/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 5600/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 5700/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 5800/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 5900/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 6000/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 6100/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 6200/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 6300/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 6400/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 6500/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 6600/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 6700/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 6800/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 6900/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 7000/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 7100/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 7200/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 7300/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 7400/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 7500/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 7600/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 7700/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 7800/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 7900/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 8000/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 8100/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 8200/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 8300/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 8400/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 8500/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 8600/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 8700/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 8800/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 8900/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 9000/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 9100/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 9200/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 9300/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 9400/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 9500/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 9600/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 9700/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 9800/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 9900/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 10000/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 10100/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 10200/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 10300/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 10400/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 10500/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 10600/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 10700/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 10800/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 10900/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 11000/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 11100/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 11200/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 11300/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 11400/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 11500/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 11600/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 11700/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 11800/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 11900/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 12000/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 12100/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 12200/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 12300/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 12400/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 12500/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 12600/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 12700/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 12800/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 12900/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 13000/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 13100/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 13200/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 13300/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 13400/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 13500/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 13600/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 13700/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 13800/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 13900/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 14000/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 14100/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 14200/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 14300/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 14400/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 14500/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 14600/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 14700/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 14800/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 14900/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 15000/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 15100/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 15200/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 15300/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 15400/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 15500/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 15600/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 15700/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 15800/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 15900/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 16000/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 16100/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 16200/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 16300/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 16400/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 16500/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 16600/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 16700/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 16800/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 16900/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 17000/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 17100/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 17200/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 17300/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 17400/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 17500/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 17600/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 17700/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 17800/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 17900/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 18000/18179\tEmbedding Shape: (128, 512)\n",
      "Previous batch took 0.12 seconds\tBatch: 18100/18179\tEmbedding Shape: (128, 512)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (179,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:29\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (179,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.eval()\n",
    "embeddings = []\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        outputs = model.encoder(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        embeddings.append(hidden_states.mean(dim=1).cpu().numpy()) # average the 256 vectors\n",
    "        \n",
    "        end = time.time()\n",
    "        i+= 1\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print(f'Previous batch took {end - start:.2f} seconds\\tBatch: {i}/{int(np.ceil(len_dataset/128))}\\tEmbedding Shape: {embeddings[-1].shape}')\n",
    "            \n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            embeddings = np.array(embeddings)\n",
    "            save_to_disk(embeddings, f'T5_embeddings/embeddings_{i}.npz')\n",
    "            embeddings = embeddings.tolist()\n",
    "            embeddings = []\n",
    "            \n",
    "if len(embeddings) > 0:\n",
    "    embeddings = np.array(embeddings)\n",
    "    save_to_disk(embeddings, f'T5_embeddings/embeddings_{i}.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 128, 512)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = np.load('T5_embeddings/embeddings_1000.npz')['arr_0']\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_to_sql_string(vector):\n",
    "    return str(vector.tolist())\n",
    "\n",
    "def from_sql_to_list(string):\n",
    "    sql_string = re.findall(r'\\[.*?\\]', string)\n",
    "    lst = [eval(i) for i in sql_string]\n",
    "    return torch.tensor(np.array(lst), device=device, dtype=torch.float64)\n",
    "\n",
    "def connect_to_db():\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        port=5432,\n",
    "        database=\"vector_database\",\n",
    "        user=\"postgres\",\n",
    "        password=\"admin\"\n",
    "    )\n",
    "    conn.autocommit = True\n",
    "\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "    return cur, conn\n",
    "\n",
    "cur, conn = connect_to_db()\n",
    "def save_batch(batch, start): \n",
    "    i = start\n",
    "    for embedding in tqdm(batch):   \n",
    "        sql_string = vec_to_sql_string(batch[i])\n",
    "        cur.execute(\"INSERT INTO article_embeddings VALUES (%s, %s)\", (i, sql_string))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"drop table article_embeddings;\")\n",
    "cur.execute(\"create table article_embeddings(article_ID int primary key, embedding vector(768));\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128000, 512)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embeddings.reshape(-1, 512)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.28093544e-02  6.75026774e-02 -2.95573473e-02 -1.31904539e-02\n",
      " -8.76235217e-02  8.76239315e-02 -1.80753209e-02 -1.09374686e-03\n",
      " -9.44832116e-02  4.24410217e-02  9.34034772e-03 -8.47868621e-02\n",
      " -3.98606472e-02  1.32916262e-03 -2.34508477e-02 -1.07465126e-03\n",
      "  9.97326337e-03  3.27166542e-02  2.37016007e-02 -5.53505942e-02\n",
      " -2.12609209e-02 -4.41676192e-03 -6.82403147e-02 -1.53395534e-01\n",
      "  1.80014856e-02  1.19663961e-02 -2.06725523e-02 -8.29009414e-02\n",
      "  1.00044971e-02 -2.83104852e-02  2.85374969e-02  2.63073388e-02\n",
      "  1.40376966e-02  2.98258886e-02 -7.50582516e-02 -2.36231964e-02\n",
      " -1.50535414e-02  1.74066275e-02  4.39158753e-02  5.58604859e-02\n",
      " -9.32354759e-03 -2.78842282e-02  2.80379742e-01 -5.09586558e-02\n",
      "  1.45210132e-01  7.65275955e-02  7.03355074e-02 -3.42450887e-02\n",
      "  7.22508878e-03 -2.05006786e-02  2.51101758e-02 -4.99866083e-02\n",
      "  9.30405874e-03  1.26727685e-01 -8.21842253e-03 -7.64768058e-03\n",
      " -1.80531219e-02 -1.04181282e-03  2.18757689e-02 -5.31628309e-03\n",
      " -1.49713242e-02  1.09140247e-01  3.26409340e-01  1.65727641e-02\n",
      "  2.56290473e-02  3.52659747e-02 -2.65005603e-02 -2.94653680e-02\n",
      "  1.37143349e-02 -2.86563151e-02 -3.79136354e-02  1.20553635e-02\n",
      " -1.85648091e-02  2.72261575e-02 -3.11617414e-03  1.81427561e-02\n",
      "  5.98585419e-03 -2.64096763e-02 -3.27315852e-02  7.01656565e-02\n",
      "  1.12214215e-01 -1.33848473e-01 -7.14788213e-03 -6.08669519e-02\n",
      "  1.94642022e-02 -3.90042588e-02 -1.48519091e-02  5.76773174e-02\n",
      "  4.00952064e-04 -2.68056896e-02  2.56866850e-02  6.38067499e-02\n",
      "  5.25980741e-02  5.56018502e-02 -4.15767170e-02 -1.49394735e-03\n",
      "  1.23332592e-03 -3.05770375e-02 -6.10251836e-02  5.27542667e-04\n",
      " -3.00676376e-03 -2.67383195e-02 -2.96427794e-02  4.54416759e-02\n",
      "  7.44536240e-03 -9.68694035e-03 -4.05037031e-02 -2.66836351e-03\n",
      " -5.72588928e-02  3.03472634e-02 -4.06517312e-02 -1.03009887e-01\n",
      " -1.14257354e-02 -3.39044705e-02  1.62670631e-02 -1.24612413e-01\n",
      "  8.56417324e-03  1.48633998e-02 -9.16771241e-04  5.80375874e-03\n",
      " -1.49653759e-02  6.19308874e-02 -3.53811793e-02  3.72313000e-02\n",
      "  1.15127116e-02  8.05719346e-02  3.15024070e-02  4.05975170e-02\n",
      "  6.06721044e-02 -5.06427400e-02 -3.60230990e-02  2.68790685e-02\n",
      "  2.34607365e-02 -8.14158842e-02 -9.94276106e-02  1.34006906e-02\n",
      " -6.91993721e-03 -6.68691173e-02  1.60501953e-02  2.63823010e-02\n",
      " -2.99487379e-03 -3.38680148e-01  1.25038838e-02 -3.74724343e-02\n",
      "  2.37227622e-02 -1.07125610e-01  1.96726546e-02  3.16141456e-01\n",
      "  9.18248147e-02 -3.67516577e-01 -4.29400653e-02 -5.95703870e-02\n",
      "  1.16867125e-01 -1.49804577e-02  1.89019851e-02  5.69765270e-03\n",
      " -8.60481430e-03 -5.35905845e-02  5.47912978e-02 -1.05584646e-03\n",
      " -2.70469729e-02  7.56915519e-03 -4.19232622e-03 -6.05766894e-04\n",
      " -4.35823724e-02 -3.04236580e-02 -5.54783568e-02  4.94492948e-02\n",
      " -2.07546204e-02  1.71119608e-02 -7.95420259e-02  3.05687124e-03\n",
      "  7.13811517e-02 -3.10467295e-02 -2.77704224e-02  1.65088773e-02\n",
      "  1.44593921e-02 -1.05099948e-02 -1.26381004e-02 -1.73847601e-02\n",
      " -2.85192728e-02 -6.39281943e-02 -1.65264949e-01 -2.87650852e-03\n",
      "  2.18257699e-02  2.76181065e-02  9.51627363e-03 -2.14731507e-02\n",
      " -8.19041114e-03 -5.47397975e-03 -5.39968302e-03 -3.24912034e-02\n",
      "  2.26326659e-02 -7.92522915e-04 -2.63603777e-02 -6.83000684e-03\n",
      " -5.24984896e-02  2.21507430e-01  1.99764166e-02 -8.31925869e-03\n",
      "  2.45786548e-01 -7.34989392e-03 -2.78349202e-02  4.78686579e-02\n",
      " -1.62932277e-02 -5.97074209e-03  6.45872205e-02  1.21451914e-01\n",
      " -4.70575243e-02 -5.14973700e-02  6.71896432e-03 -9.26689059e-03\n",
      " -3.35640162e-02  8.24265182e-02 -1.01081878e-02  2.95177381e-03\n",
      "  7.45967403e-03 -5.67537285e-02 -4.83112782e-02  1.85701856e-03\n",
      " -8.06032121e-03 -1.08665004e-02  9.00356099e-02 -1.54383220e-02\n",
      " -4.42701727e-02 -9.54463612e-03  7.71964118e-02 -1.78737007e-02\n",
      "  6.01166859e-02  2.10215628e-01  1.99279515e-04 -2.49030627e-02\n",
      " -2.38137320e-04 -7.37556368e-02 -6.45225048e-02  2.76337843e-02\n",
      " -8.33997503e-02 -2.79450361e-02  5.23833036e-02 -6.30765855e-02\n",
      " -1.43196136e-02  5.33141345e-02 -2.46942956e-02 -4.01073843e-02\n",
      "  2.08953163e-04  6.65611476e-02 -9.75175798e-02  1.17768005e-01\n",
      "  9.54174548e-02 -8.03919695e-03  3.63857523e-02  3.22549380e-02\n",
      "  2.22618375e-02 -1.03930645e-02 -4.38570604e-03  4.18983661e-02\n",
      "  4.79025096e-02  3.62204164e-02  3.50868739e-02 -1.02374598e-01\n",
      " -6.30023628e-02  5.53514296e-03  2.11170153e-03  1.31301478e-01\n",
      " -4.20125276e-02 -2.54800618e-02 -2.60676295e-02 -2.94810440e-03\n",
      " -2.70647053e-02 -3.56047601e-02 -3.93281365e-03 -7.06906570e-03\n",
      "  5.82488701e-02 -4.02657509e-01 -9.66496486e-03 -2.20085233e-02\n",
      "  1.60165653e-02  5.69255874e-02  3.72239808e-03  3.38858478e-02\n",
      "  2.61645559e-02  3.93365398e-02  9.43002477e-03  9.05619860e-02\n",
      " -1.69677213e-02 -1.95353776e-02 -1.48022221e-02 -1.10453367e-01\n",
      "  4.72208709e-02 -2.06235563e-04 -8.09047222e-02 -8.52545723e-03\n",
      "  1.44735519e-02  3.11467564e-03 -7.13246763e-02  4.61202376e-02\n",
      " -1.33029167e-02 -5.11753634e-02  1.52567988e-02  1.56362101e-01\n",
      " -5.57884797e-02  3.95660754e-05  1.09800175e-01  1.16834920e-02\n",
      "  9.18595586e-03 -3.25929783e-02 -3.00596133e-02  7.87284691e-03\n",
      "  2.43331343e-02 -5.15130833e-02 -1.70075111e-02  2.29187049e-02\n",
      " -6.36904985e-02  5.10938279e-02 -1.43283252e-02 -3.95964347e-02\n",
      " -2.20506899e-02  2.55861925e-03  3.01155280e-02  3.34083661e-02\n",
      "  2.84386724e-02 -3.20965834e-02 -1.14831366e-02 -7.35092610e-02\n",
      "  7.61001110e-02  1.29525596e-02  4.39958554e-03 -3.10073495e-02\n",
      "  1.64901502e-02 -4.36159549e-04 -7.35100638e-03 -1.10395178e-01\n",
      " -4.34199907e-02 -3.81970871e-03 -1.30076353e-02 -3.57270357e-03\n",
      "  4.86979447e-02  4.73333709e-03  6.65809959e-03  3.82780671e-01\n",
      " -2.33278722e-02 -4.45538089e-02 -5.50544858e-02 -1.02096856e-01\n",
      " -1.05475960e-02 -2.95557603e-02 -1.34783328e-01 -9.91567224e-03\n",
      " -3.76532599e-02 -2.19823107e-01  1.53788263e-02 -3.79910739e-03\n",
      " -1.60449613e-02 -1.63904317e-02  1.75908692e-02  4.44622412e-02\n",
      " -4.68284190e-02  1.44293876e-02 -4.54554567e-03 -8.99182633e-03\n",
      "  3.51225175e-02  1.43582560e-02  1.83865391e-02 -3.60687748e-02\n",
      " -2.00380795e-02 -9.70546454e-02  3.10238600e-02 -2.61582807e-02\n",
      "  2.28375681e-02 -2.11326592e-02 -4.12551090e-02  2.40632370e-02\n",
      " -2.57956516e-02 -5.49292788e-02  2.42677238e-02  1.53768081e-02\n",
      "  1.89679749e-02  3.34101655e-02  8.83794390e-03  5.58237024e-02\n",
      " -1.70653723e-02 -2.14883629e-02 -1.52449962e-02 -1.83403138e-02\n",
      "  1.39570283e-02 -5.31469434e-02  5.11570275e-02  4.84054908e-02\n",
      " -2.66047269e-02 -2.02228944e-03  1.75153725e-02  2.56681964e-02\n",
      "  4.37939279e-02 -2.17513442e-02  2.08041398e-04  4.42078635e-02\n",
      " -5.81040867e-02  9.31248814e-03  2.61638388e-02 -1.73146665e-01\n",
      " -8.04536268e-02 -2.36178981e-03 -7.63377221e-03  7.45999888e-02\n",
      "  5.26217604e-03 -2.37449370e-02  3.04826945e-02 -1.70112941e-02\n",
      "  4.99197375e-03 -2.64751469e-03 -1.58283159e-01 -4.97081503e-02\n",
      "  9.41626634e-03  6.33275509e-02 -3.31710726e-02  1.34496558e-02\n",
      "  6.90277964e-02  1.00864634e-01 -1.85155794e-02 -3.49822938e-02\n",
      " -6.23729359e-03 -3.13530266e-02  4.75101918e-02  2.42147893e-02\n",
      " -2.59253979e-02  2.60662399e-02  4.57471237e-02  6.56016869e-03\n",
      " -1.36922486e-03 -5.41164428e-02  1.54851936e-02 -1.64133385e-02\n",
      " -1.34856831e-02  4.37948033e-02  5.19152209e-02 -2.52184756e-02\n",
      "  8.03799778e-02 -5.68628684e-03  3.33301127e-02  7.83119537e-03\n",
      " -2.17151698e-02 -2.39413530e-02 -9.68466746e-04  7.99656101e-03\n",
      "  2.10480466e-02  5.65268993e-02  3.32736522e-02 -5.03358096e-02\n",
      "  2.87387315e-02  2.76541114e-02 -3.36388461e-02 -8.53104331e-03\n",
      "  3.17669585e-02 -9.58321914e-02 -3.20449769e-02  3.41013819e-02\n",
      " -4.51145694e-02  3.51217315e-02  4.23897104e-03 -3.27387452e-02\n",
      " -2.03214101e-02  2.72940658e-02  5.05978167e-02 -5.15280515e-02\n",
      "  1.48341302e-02  3.82049079e-03 -5.06678829e-03 -2.17679143e-01\n",
      " -5.35607301e-02 -1.80827323e-02  7.07731117e-03 -8.32441449e-03\n",
      " -5.68501353e-02  3.04904021e-03  5.40479384e-02  7.52648711e-02\n",
      " -5.83220506e-03 -6.71032816e-02  3.44208675e-03 -8.08527246e-02\n",
      " -6.41482547e-02 -1.32013690e-02 -2.48428676e-02 -1.52996471e-02\n",
      " -2.10932065e-02  2.01057084e-02  2.32896991e-02 -1.94438323e-02\n",
      " -8.03293753e-03 -2.62609497e-02  7.55119696e-02  9.03173238e-02\n",
      "  1.34760648e-01 -1.07710566e-02  1.57814417e-02 -6.05333224e-02\n",
      " -5.40157333e-02  1.78141594e-02 -1.52241206e-02 -2.82562859e-02\n",
      "  6.74650967e-02  5.88568188e-02 -3.42791453e-02  3.79909761e-02\n",
      " -1.96058024e-03 -3.85206401e-01 -4.38590720e-02 -2.29934286e-02\n",
      "  2.20258310e-02 -3.24034803e-02 -1.66472252e-02  7.73852086e-03]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(embeddings[0])\n",
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128000\n"
     ]
    }
   ],
   "source": [
    "index = faiss.IndexFlatL2(512)\n",
    "index.add(embeddings)\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x00000267CA986AF0> >"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(embeddings[:1], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.19465011 0.19810846 0.20966665]]\n",
      "[[     0  25322  65998 112867]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.03280935,  0.06750268, -0.02955735, ..., -0.03240348,\n",
       "        -0.01664723,  0.00773852],\n",
       "       [ 0.01696098,  0.10538723, -0.0293349 , ..., -0.03645744,\n",
       "        -0.02133687,  0.0337696 ],\n",
       "       [ 0.00078729,  0.06435277, -0.02840437, ..., -0.05428283,\n",
       "        -0.02219047,  0.02471812],\n",
       "       [-0.00676448,  0.08924416, -0.01654493, ..., -0.03034954,\n",
       "        -0.01020308,  0.0657523 ]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explain what D and I are\n",
    "print(D)\n",
    "print(I)\n",
    "\n",
    "# get the vectors for the first 4 nearest neighbors\n",
    "embeddings[I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_embeddings():\n",
    "    e_list = []\n",
    "    for i in range(1, 19):\n",
    "        embeddings = np.load(f'T5_embeddings/embeddings_{i}000.npz')['arr_0'].reshape(-1, 512)\n",
    "        e_list.append(embeddings)\n",
    "    \n",
    "    e = np.concatenate(e_list, axis=0)\n",
    "    return e\n",
    "\n",
    "embeddings_array = load_all_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2304000, 512)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304000\n",
      "CPU times: total: 219 ms\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "index = faiss.IndexFlatL2(512)\n",
    "index.add(embeddings_array)\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, 'Indexes/T5_embeddings.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_index = faiss.read_index('Indexes/T5_embeddings.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert loaded_index.ntotal == 2304000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = loaded_index.search(embeddings_array[:1], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0.        , 0.18301383, 0.1832258 , 0.18500452]], dtype=float32)\n",
      "array([[      0,  585399, 1387869,  336090]], dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.03280935,  0.06750268, -0.02955735, ..., -0.03240348,\n",
       "        -0.01664723,  0.00773852],\n",
       "       [ 0.00789752,  0.09153082, -0.04614971, ..., -0.03393538,\n",
       "        -0.03743657,  0.01426536],\n",
       "       [-0.00270015,  0.08274905, -0.00822058, ..., -0.01871685,\n",
       "        -0.03101622, -0.00574923],\n",
       "       [ 0.02184535,  0.09049194, -0.03042706, ..., -0.02963874,\n",
       "        -0.02268996,  0.05477721]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pprint(D)\n",
    "pprint(I)\n",
    "\n",
    "embeddings_array[I[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
