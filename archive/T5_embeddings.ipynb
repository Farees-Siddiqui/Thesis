{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import time\n",
    "\n",
    "import sys\n",
    "\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import faiss\n",
    "\n",
    "from transformers import BertTokenizerFast, BertModel, T5TokenizerFast, T5Model\n",
    "from datasets import load_dataset\n",
    "\n",
    "from pprint import pprint\n",
    "import io\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helper import stream\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "len_dataset = 2326839\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Model(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = T5TokenizerFast.from_pretrained('t5-small')\n",
    "model = T5Model.from_pretrained('t5-small', output_hidden_states=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('json', data_files='dataset/arxiv_data.json', split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.iterable_dataset.IterableDataset at 0x2699b4771d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0704.0001',\n",
       " 'submitter': 'Pavel Nadolsky',\n",
       " 'authors': \"C. Bal\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\",\n",
       " 'title': 'Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies',\n",
       " 'comments': '37 pages, 15 figures; published version',\n",
       " 'journal-ref': 'Phys.Rev.D76:013009,2007',\n",
       " 'doi': '10.1103/PhysRevD.76.013009',\n",
       " 'report-no': 'ANL-HEP-PR-07-12',\n",
       " 'categories': 'hep-ph',\n",
       " 'license': None,\n",
       " 'abstract': '  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n',\n",
       " 'versions': [{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'},\n",
       "  {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:27 GMT'}],\n",
       " 'update_date': datetime.datetime(2008, 11, 26, 0, 0),\n",
       " 'authors_parsed': [['Bal√°zs', 'C.', ''],\n",
       "  ['Berger', 'E. L.', ''],\n",
       "  ['Nadolsky', 'P. M.', ''],\n",
       "  ['Yuan', 'C. -P.', '']]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(data):\n",
    "    return tokenizer(data['abstract'], padding=True, truncation=True, max_length=256, return_tensors='pt')\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_dataset, batched=True, batch_size=512, remove_columns=['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'versions', 'update_date', 'authors_parsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(tokenized_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(next(iter(dataloader))['abstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_disk(data, filename):\n",
    "    np.savez(filename, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Model(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:11\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\faree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\faree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1090\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1077\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[0;32m   1078\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m   1079\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1087\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     )\n\u001b[0;32m   1089\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1090\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m   1091\u001b[0m         hidden_states,\n\u001b[0;32m   1092\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1093\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[0;32m   1094\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1095\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1096\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[0;32m   1097\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m   1098\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[0;32m   1099\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m   1100\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1101\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1102\u001b[0m     )\n\u001b[0;32m   1104\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   1105\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\faree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\faree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:693\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    691\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 693\u001b[0m self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m0\u001b[39;49m](\n\u001b[0;32m    694\u001b[0m     hidden_states,\n\u001b[0;32m    695\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    696\u001b[0m     position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[0;32m    697\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m    698\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    699\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    700\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    701\u001b[0m )\n\u001b[0;32m    702\u001b[0m hidden_states, present_key_value_state \u001b[39m=\u001b[39m self_attention_outputs[:\u001b[39m2\u001b[39m]\n\u001b[0;32m    703\u001b[0m attention_outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m2\u001b[39m:]  \u001b[39m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\faree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\faree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:600\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    590\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    591\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    597\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    598\u001b[0m ):\n\u001b[0;32m    599\u001b[0m     normed_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 600\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mSelfAttention(\n\u001b[0;32m    601\u001b[0m         normed_hidden_states,\n\u001b[0;32m    602\u001b[0m         mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    603\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[0;32m    604\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m    605\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m    606\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    607\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    608\u001b[0m     )\n\u001b[0;32m    609\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_output[\u001b[39m0\u001b[39m])\n\u001b[0;32m    610\u001b[0m     outputs \u001b[39m=\u001b[39m (hidden_states,) \u001b[39m+\u001b[39m attention_output[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\faree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\faree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:519\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[1;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n\u001b[0;32m    518\u001b[0m \u001b[39m# get query states\u001b[39;00m\n\u001b[1;32m--> 519\u001b[0m query_states \u001b[39m=\u001b[39m shape(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq(hidden_states))  \u001b[39m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[39m# get key/value states\u001b[39;00m\n\u001b[0;32m    522\u001b[0m key_states \u001b[39m=\u001b[39m project(\n\u001b[0;32m    523\u001b[0m     hidden_states, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk, key_value_states, past_key_value[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\faree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\faree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.eval()\n",
    "embeddings = []\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        outputs = model.encoder(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        embeddings.append(hidden_states.mean(dim=1).cpu().numpy()) # average the 256 vectors\n",
    "        \n",
    "        end = time.time()\n",
    "        i+= 1\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print(f'Previous batch took {end - start:.2f} seconds\\tBatch: {i}/{int(np.ceil(len_dataset/128))}\\tEmbedding Shape: {embeddings[-1].shape}')\n",
    "            \n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            embeddings = np.array(embeddings)\n",
    "            save_to_disk(embeddings, f'T5_embeddings/embeddings_{i}.npz')\n",
    "            embeddings = embeddings.tolist()\n",
    "            embeddings = []\n",
    "            \n",
    "if len(embeddings) > 0:\n",
    "    embeddings = np.array(embeddings)\n",
    "    save_to_disk(embeddings, f'T5_embeddings/embeddings_{i}.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 128, 512)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = np.load('T5_embeddings/embeddings_1000.npz')['arr_0']\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_to_sql_string(vector):\n",
    "    return str(vector.tolist())\n",
    "\n",
    "def from_sql_to_list(string):\n",
    "    sql_string = re.findall(r'\\[.*?\\]', string)\n",
    "    lst = [eval(i) for i in sql_string]\n",
    "    return torch.tensor(np.array(lst), device=device, dtype=torch.float64)\n",
    "\n",
    "def connect_to_db():\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        port=5432,\n",
    "        database=\"vector_database\",\n",
    "        user=\"postgres\",\n",
    "        password=\"admin\"\n",
    "    )\n",
    "    conn.autocommit = True\n",
    "\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "    return cur, conn\n",
    "\n",
    "cur, conn = connect_to_db()\n",
    "def save_batch(batch): \n",
    "    i = 0\n",
    "    for embedding in tqdm(batch):   \n",
    "        sql_string = vec_to_sql_string(batch[i])\n",
    "        cur.execute(\"INSERT INTO T5_article_embeddings VALUES (%s, %s)\", (i, sql_string))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"drop table T5_article_embeddings;\")\n",
    "cur.execute(\"create table T5_article_embeddings(article_ID int primary key, embedding vector(512));\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128000, 512)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embeddings.reshape(-1, 512)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.28093544e-02  6.75026774e-02 -2.95573473e-02 -1.31904539e-02\n",
      " -8.76235217e-02  8.76239315e-02 -1.80753209e-02 -1.09374686e-03\n",
      " -9.44832116e-02  4.24410217e-02  9.34034772e-03 -8.47868621e-02\n",
      " -3.98606472e-02  1.32916262e-03 -2.34508477e-02 -1.07465126e-03\n",
      "  9.97326337e-03  3.27166542e-02  2.37016007e-02 -5.53505942e-02\n",
      " -2.12609209e-02 -4.41676192e-03 -6.82403147e-02 -1.53395534e-01\n",
      "  1.80014856e-02  1.19663961e-02 -2.06725523e-02 -8.29009414e-02\n",
      "  1.00044971e-02 -2.83104852e-02  2.85374969e-02  2.63073388e-02\n",
      "  1.40376966e-02  2.98258886e-02 -7.50582516e-02 -2.36231964e-02\n",
      " -1.50535414e-02  1.74066275e-02  4.39158753e-02  5.58604859e-02\n",
      " -9.32354759e-03 -2.78842282e-02  2.80379742e-01 -5.09586558e-02\n",
      "  1.45210132e-01  7.65275955e-02  7.03355074e-02 -3.42450887e-02\n",
      "  7.22508878e-03 -2.05006786e-02  2.51101758e-02 -4.99866083e-02\n",
      "  9.30405874e-03  1.26727685e-01 -8.21842253e-03 -7.64768058e-03\n",
      " -1.80531219e-02 -1.04181282e-03  2.18757689e-02 -5.31628309e-03\n",
      " -1.49713242e-02  1.09140247e-01  3.26409340e-01  1.65727641e-02\n",
      "  2.56290473e-02  3.52659747e-02 -2.65005603e-02 -2.94653680e-02\n",
      "  1.37143349e-02 -2.86563151e-02 -3.79136354e-02  1.20553635e-02\n",
      " -1.85648091e-02  2.72261575e-02 -3.11617414e-03  1.81427561e-02\n",
      "  5.98585419e-03 -2.64096763e-02 -3.27315852e-02  7.01656565e-02\n",
      "  1.12214215e-01 -1.33848473e-01 -7.14788213e-03 -6.08669519e-02\n",
      "  1.94642022e-02 -3.90042588e-02 -1.48519091e-02  5.76773174e-02\n",
      "  4.00952064e-04 -2.68056896e-02  2.56866850e-02  6.38067499e-02\n",
      "  5.25980741e-02  5.56018502e-02 -4.15767170e-02 -1.49394735e-03\n",
      "  1.23332592e-03 -3.05770375e-02 -6.10251836e-02  5.27542667e-04\n",
      " -3.00676376e-03 -2.67383195e-02 -2.96427794e-02  4.54416759e-02\n",
      "  7.44536240e-03 -9.68694035e-03 -4.05037031e-02 -2.66836351e-03\n",
      " -5.72588928e-02  3.03472634e-02 -4.06517312e-02 -1.03009887e-01\n",
      " -1.14257354e-02 -3.39044705e-02  1.62670631e-02 -1.24612413e-01\n",
      "  8.56417324e-03  1.48633998e-02 -9.16771241e-04  5.80375874e-03\n",
      " -1.49653759e-02  6.19308874e-02 -3.53811793e-02  3.72313000e-02\n",
      "  1.15127116e-02  8.05719346e-02  3.15024070e-02  4.05975170e-02\n",
      "  6.06721044e-02 -5.06427400e-02 -3.60230990e-02  2.68790685e-02\n",
      "  2.34607365e-02 -8.14158842e-02 -9.94276106e-02  1.34006906e-02\n",
      " -6.91993721e-03 -6.68691173e-02  1.60501953e-02  2.63823010e-02\n",
      " -2.99487379e-03 -3.38680148e-01  1.25038838e-02 -3.74724343e-02\n",
      "  2.37227622e-02 -1.07125610e-01  1.96726546e-02  3.16141456e-01\n",
      "  9.18248147e-02 -3.67516577e-01 -4.29400653e-02 -5.95703870e-02\n",
      "  1.16867125e-01 -1.49804577e-02  1.89019851e-02  5.69765270e-03\n",
      " -8.60481430e-03 -5.35905845e-02  5.47912978e-02 -1.05584646e-03\n",
      " -2.70469729e-02  7.56915519e-03 -4.19232622e-03 -6.05766894e-04\n",
      " -4.35823724e-02 -3.04236580e-02 -5.54783568e-02  4.94492948e-02\n",
      " -2.07546204e-02  1.71119608e-02 -7.95420259e-02  3.05687124e-03\n",
      "  7.13811517e-02 -3.10467295e-02 -2.77704224e-02  1.65088773e-02\n",
      "  1.44593921e-02 -1.05099948e-02 -1.26381004e-02 -1.73847601e-02\n",
      " -2.85192728e-02 -6.39281943e-02 -1.65264949e-01 -2.87650852e-03\n",
      "  2.18257699e-02  2.76181065e-02  9.51627363e-03 -2.14731507e-02\n",
      " -8.19041114e-03 -5.47397975e-03 -5.39968302e-03 -3.24912034e-02\n",
      "  2.26326659e-02 -7.92522915e-04 -2.63603777e-02 -6.83000684e-03\n",
      " -5.24984896e-02  2.21507430e-01  1.99764166e-02 -8.31925869e-03\n",
      "  2.45786548e-01 -7.34989392e-03 -2.78349202e-02  4.78686579e-02\n",
      " -1.62932277e-02 -5.97074209e-03  6.45872205e-02  1.21451914e-01\n",
      " -4.70575243e-02 -5.14973700e-02  6.71896432e-03 -9.26689059e-03\n",
      " -3.35640162e-02  8.24265182e-02 -1.01081878e-02  2.95177381e-03\n",
      "  7.45967403e-03 -5.67537285e-02 -4.83112782e-02  1.85701856e-03\n",
      " -8.06032121e-03 -1.08665004e-02  9.00356099e-02 -1.54383220e-02\n",
      " -4.42701727e-02 -9.54463612e-03  7.71964118e-02 -1.78737007e-02\n",
      "  6.01166859e-02  2.10215628e-01  1.99279515e-04 -2.49030627e-02\n",
      " -2.38137320e-04 -7.37556368e-02 -6.45225048e-02  2.76337843e-02\n",
      " -8.33997503e-02 -2.79450361e-02  5.23833036e-02 -6.30765855e-02\n",
      " -1.43196136e-02  5.33141345e-02 -2.46942956e-02 -4.01073843e-02\n",
      "  2.08953163e-04  6.65611476e-02 -9.75175798e-02  1.17768005e-01\n",
      "  9.54174548e-02 -8.03919695e-03  3.63857523e-02  3.22549380e-02\n",
      "  2.22618375e-02 -1.03930645e-02 -4.38570604e-03  4.18983661e-02\n",
      "  4.79025096e-02  3.62204164e-02  3.50868739e-02 -1.02374598e-01\n",
      " -6.30023628e-02  5.53514296e-03  2.11170153e-03  1.31301478e-01\n",
      " -4.20125276e-02 -2.54800618e-02 -2.60676295e-02 -2.94810440e-03\n",
      " -2.70647053e-02 -3.56047601e-02 -3.93281365e-03 -7.06906570e-03\n",
      "  5.82488701e-02 -4.02657509e-01 -9.66496486e-03 -2.20085233e-02\n",
      "  1.60165653e-02  5.69255874e-02  3.72239808e-03  3.38858478e-02\n",
      "  2.61645559e-02  3.93365398e-02  9.43002477e-03  9.05619860e-02\n",
      " -1.69677213e-02 -1.95353776e-02 -1.48022221e-02 -1.10453367e-01\n",
      "  4.72208709e-02 -2.06235563e-04 -8.09047222e-02 -8.52545723e-03\n",
      "  1.44735519e-02  3.11467564e-03 -7.13246763e-02  4.61202376e-02\n",
      " -1.33029167e-02 -5.11753634e-02  1.52567988e-02  1.56362101e-01\n",
      " -5.57884797e-02  3.95660754e-05  1.09800175e-01  1.16834920e-02\n",
      "  9.18595586e-03 -3.25929783e-02 -3.00596133e-02  7.87284691e-03\n",
      "  2.43331343e-02 -5.15130833e-02 -1.70075111e-02  2.29187049e-02\n",
      " -6.36904985e-02  5.10938279e-02 -1.43283252e-02 -3.95964347e-02\n",
      " -2.20506899e-02  2.55861925e-03  3.01155280e-02  3.34083661e-02\n",
      "  2.84386724e-02 -3.20965834e-02 -1.14831366e-02 -7.35092610e-02\n",
      "  7.61001110e-02  1.29525596e-02  4.39958554e-03 -3.10073495e-02\n",
      "  1.64901502e-02 -4.36159549e-04 -7.35100638e-03 -1.10395178e-01\n",
      " -4.34199907e-02 -3.81970871e-03 -1.30076353e-02 -3.57270357e-03\n",
      "  4.86979447e-02  4.73333709e-03  6.65809959e-03  3.82780671e-01\n",
      " -2.33278722e-02 -4.45538089e-02 -5.50544858e-02 -1.02096856e-01\n",
      " -1.05475960e-02 -2.95557603e-02 -1.34783328e-01 -9.91567224e-03\n",
      " -3.76532599e-02 -2.19823107e-01  1.53788263e-02 -3.79910739e-03\n",
      " -1.60449613e-02 -1.63904317e-02  1.75908692e-02  4.44622412e-02\n",
      " -4.68284190e-02  1.44293876e-02 -4.54554567e-03 -8.99182633e-03\n",
      "  3.51225175e-02  1.43582560e-02  1.83865391e-02 -3.60687748e-02\n",
      " -2.00380795e-02 -9.70546454e-02  3.10238600e-02 -2.61582807e-02\n",
      "  2.28375681e-02 -2.11326592e-02 -4.12551090e-02  2.40632370e-02\n",
      " -2.57956516e-02 -5.49292788e-02  2.42677238e-02  1.53768081e-02\n",
      "  1.89679749e-02  3.34101655e-02  8.83794390e-03  5.58237024e-02\n",
      " -1.70653723e-02 -2.14883629e-02 -1.52449962e-02 -1.83403138e-02\n",
      "  1.39570283e-02 -5.31469434e-02  5.11570275e-02  4.84054908e-02\n",
      " -2.66047269e-02 -2.02228944e-03  1.75153725e-02  2.56681964e-02\n",
      "  4.37939279e-02 -2.17513442e-02  2.08041398e-04  4.42078635e-02\n",
      " -5.81040867e-02  9.31248814e-03  2.61638388e-02 -1.73146665e-01\n",
      " -8.04536268e-02 -2.36178981e-03 -7.63377221e-03  7.45999888e-02\n",
      "  5.26217604e-03 -2.37449370e-02  3.04826945e-02 -1.70112941e-02\n",
      "  4.99197375e-03 -2.64751469e-03 -1.58283159e-01 -4.97081503e-02\n",
      "  9.41626634e-03  6.33275509e-02 -3.31710726e-02  1.34496558e-02\n",
      "  6.90277964e-02  1.00864634e-01 -1.85155794e-02 -3.49822938e-02\n",
      " -6.23729359e-03 -3.13530266e-02  4.75101918e-02  2.42147893e-02\n",
      " -2.59253979e-02  2.60662399e-02  4.57471237e-02  6.56016869e-03\n",
      " -1.36922486e-03 -5.41164428e-02  1.54851936e-02 -1.64133385e-02\n",
      " -1.34856831e-02  4.37948033e-02  5.19152209e-02 -2.52184756e-02\n",
      "  8.03799778e-02 -5.68628684e-03  3.33301127e-02  7.83119537e-03\n",
      " -2.17151698e-02 -2.39413530e-02 -9.68466746e-04  7.99656101e-03\n",
      "  2.10480466e-02  5.65268993e-02  3.32736522e-02 -5.03358096e-02\n",
      "  2.87387315e-02  2.76541114e-02 -3.36388461e-02 -8.53104331e-03\n",
      "  3.17669585e-02 -9.58321914e-02 -3.20449769e-02  3.41013819e-02\n",
      " -4.51145694e-02  3.51217315e-02  4.23897104e-03 -3.27387452e-02\n",
      " -2.03214101e-02  2.72940658e-02  5.05978167e-02 -5.15280515e-02\n",
      "  1.48341302e-02  3.82049079e-03 -5.06678829e-03 -2.17679143e-01\n",
      " -5.35607301e-02 -1.80827323e-02  7.07731117e-03 -8.32441449e-03\n",
      " -5.68501353e-02  3.04904021e-03  5.40479384e-02  7.52648711e-02\n",
      " -5.83220506e-03 -6.71032816e-02  3.44208675e-03 -8.08527246e-02\n",
      " -6.41482547e-02 -1.32013690e-02 -2.48428676e-02 -1.52996471e-02\n",
      " -2.10932065e-02  2.01057084e-02  2.32896991e-02 -1.94438323e-02\n",
      " -8.03293753e-03 -2.62609497e-02  7.55119696e-02  9.03173238e-02\n",
      "  1.34760648e-01 -1.07710566e-02  1.57814417e-02 -6.05333224e-02\n",
      " -5.40157333e-02  1.78141594e-02 -1.52241206e-02 -2.82562859e-02\n",
      "  6.74650967e-02  5.88568188e-02 -3.42791453e-02  3.79909761e-02\n",
      " -1.96058024e-03 -3.85206401e-01 -4.38590720e-02 -2.29934286e-02\n",
      "  2.20258310e-02 -3.24034803e-02 -1.66472252e-02  7.73852086e-03]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(embeddings[0])\n",
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128000\n"
     ]
    }
   ],
   "source": [
    "index = faiss.IndexFlatL2(512)\n",
    "index.add(embeddings)\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x00000247D90CF300> >"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(embeddings[:1], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.19465011 0.19810846 0.20966665]]\n",
      "[[     0  25322  65998 112867]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.03280935,  0.06750268, -0.02955735, ..., -0.03240348,\n",
       "        -0.01664723,  0.00773852],\n",
       "       [ 0.01696098,  0.10538723, -0.0293349 , ..., -0.03645744,\n",
       "        -0.02133687,  0.0337696 ],\n",
       "       [ 0.00078729,  0.06435277, -0.02840437, ..., -0.05428283,\n",
       "        -0.02219047,  0.02471812],\n",
       "       [-0.00676448,  0.08924416, -0.01654493, ..., -0.03034954,\n",
       "        -0.01020308,  0.0657523 ]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explain what D and I are\n",
    "print(D)\n",
    "print(I)\n",
    "\n",
    "# get the vectors for the first 4 nearest neighbors\n",
    "embeddings[I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_embeddings():\n",
    "    e_list = []\n",
    "    for i in range(1, 19):\n",
    "        embeddings = np.load(f'bert_embeddings/embeddings_{i}000.npz')['arr_0'].reshape(-1, 768)\n",
    "        e_list.append(embeddings)\n",
    "    \n",
    "    e = np.concatenate(e_list, axis=0)\n",
    "    return e\n",
    "\n",
    "embeddings_array = load_all_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3456000, 512)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304000\n",
      "CPU times: total: 500 ms\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "index = faiss.IndexFlatL2(512)\n",
    "index.add(embeddings_array)\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, 'Indexes/T5_embeddings.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_index = faiss.read_index('Indexes/T5_embeddings.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert loaded_index.ntotal == 2304000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = loaded_index.search(embeddings_array[:1], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0.        , 0.18301383, 0.1832258 , 0.18500452]], dtype=float32)\n",
      "array([[      0,  585399, 1387869,  336090]], dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.03280935,  0.06750268, -0.02955735, ..., -0.03240348,\n",
       "        -0.01664723,  0.00773852],\n",
       "       [ 0.00789752,  0.09153082, -0.04614971, ..., -0.03393538,\n",
       "        -0.03743657,  0.01426536],\n",
       "       [-0.00270015,  0.08274905, -0.00822058, ..., -0.01871685,\n",
       "        -0.03101622, -0.00574923],\n",
       "       [ 0.02184535,  0.09049194, -0.03042706, ..., -0.02963874,\n",
       "        -0.02268996,  0.05477721]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pprint(D)\n",
    "pprint(I)\n",
    "\n",
    "embeddings_array[I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2304000, 512)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 133/2304000 [00:06<32:08:42, 19.91it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\faree\\Documents\\School\\Thesis\\GeneratingEmbeddings\\T5_embeddings.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/faree/Documents/School/Thesis/GeneratingEmbeddings/T5_embeddings.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m save_batch(embeddings_array)\n",
      "\u001b[1;32mc:\\Users\\faree\\Documents\\School\\Thesis\\GeneratingEmbeddings\\T5_embeddings.ipynb Cell 31\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/faree/Documents/School/Thesis/GeneratingEmbeddings/T5_embeddings.ipynb#X42sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m embedding \u001b[39min\u001b[39;00m tqdm(batch):   \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/faree/Documents/School/Thesis/GeneratingEmbeddings/T5_embeddings.ipynb#X42sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     sql_string \u001b[39m=\u001b[39m vec_to_sql_string(batch[i])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/faree/Documents/School/Thesis/GeneratingEmbeddings/T5_embeddings.ipynb#X42sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     cur\u001b[39m.\u001b[39;49mexecute(\u001b[39m\"\u001b[39;49m\u001b[39mINSERT INTO T5_article_embeddings VALUES (\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m, \u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m)\u001b[39;49m\u001b[39m\"\u001b[39;49m, (i, sql_string))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/faree/Documents/School/Thesis/GeneratingEmbeddings/T5_embeddings.ipynb#X42sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\faree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\encodings\\utf_8.py:15\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(input, errors)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m### Codec APIs\u001b[39;00m\n\u001b[0;32m     13\u001b[0m encode \u001b[39m=\u001b[39m codecs\u001b[39m.\u001b[39mutf_8_encode\n\u001b[1;32m---> 15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39minput\u001b[39m, errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39mutf_8_decode(\u001b[39minput\u001b[39m, errors, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mIncrementalEncoder\u001b[39;00m(codecs\u001b[39m.\u001b[39mIncrementalEncoder):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_batch(embeddings_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
